{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture: Tests\n",
    "\n",
    "**Week 11**\n",
    "\n",
    "- based on Martin's lecture\n",
    "  \n",
    "[Testing](#Testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "* many ways to test code\n",
    "* you've all done an exploratory/manual testing\n",
    "* to cover the whole codebase with manual tests, it is necessary:\n",
    "    * list all the code/projects features\n",
    "    * collect all (different) types of inputs it \n",
    "    * collect the corresponding expected results\n",
    "* !problem: change in code → change the above\n",
    "    * not fun → **automated testing**\n",
    "        * running test from script instead of manually\n",
    "   \n",
    "* 2 main test categories:\n",
    "    * integration tests - testing multiple if multiple components work together\n",
    "    * unit tests - testing a single component\n",
    "\n",
    "* (most) functional tests consist of:\n",
    "    1. **Arrange** - conditions in/for which we test\n",
    "    2. **Act** - running the behaviour we want to test\n",
    "    3. **Assert** - check if behaviour produced expected result\n",
    "    4. **Cleanup** - don't influence other tests\n",
    "\n",
    "* the most basic test can be done using `assert` method\n",
    "    * e.g. lets check/test if `len` method is the same as `__len__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = [1,2,3,5] \n",
    "assert len(a_list) == a_list.__len__(), \"Function len returned different result than method __len__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* we could try different data-structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tuple = (1,2,3,5)\n",
    "assert len(a_tuple) == a_tuple.__len__(), \"Function len returned different result than method __len__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tuple = (1,2,3,5)\n",
    "assert len(a_tuple) == a_tuple.__len__(), \"Function len returned different result than method __len__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum([1,1]) == 3, '2. Your result is off.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* instead of testing on the REPL, we can put our tests into a test script and run it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load test_1.py\n",
    "def test_sum():\n",
    "    assert sum([1,1]) == 2, \"Should be 2\"\n",
    "    \n",
    "def test_len_vs__len__():\n",
    "    a_tuple = (1,2,3,5)\n",
    "    assert len(a_tuple) == a_tuple.__len__(), \"Function len returned differnt result than method __len__\"\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    test_sum()\n",
    "    test_len_vs__len__()\n",
    "    print('All tests passed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run test_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* OK for simple check, cumbersome for more tests\n",
    "    * → **test runners**\n",
    "* test runner = application designed for running tests\n",
    "    * check the output\n",
    "    * offer tools for diagnosing\n",
    "    \n",
    "* many test runners available for Python\n",
    "    * *unittest* (built into the Python standard library)\n",
    "    * nose/nose2\n",
    "    * doctest\n",
    "    * robot\n",
    "    * **pytest**, ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a framework for building simple and scalable tests\n",
    "* one of the most popular Python testing frameworks\n",
    "    * feature-rich\n",
    "    * a lot of available [plugins](https://docs.pytest.org/en/latest/reference/plugin_list.html)\n",
    " \n",
    "* pytest works with the simple assert statements\n",
    "    * not necessarily the case with other test runners\n",
    "\n",
    "* how does pytest know which tests to run?\n",
    "    * by default it runs all files of the form `test_*.py` or `*_test.py` in the current directory and subdirectories\n",
    "        * however check [conventions for test discovery rules](https://docs.pytest.org/en/6.2.x/goodpractices.html#test-discovery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* what does it tell us:\n",
    "    * the system tests are run on (Python, pytest version, and any pluggins\n",
    "    * *rootdir* : where are we running things from\n",
    "    * [XX%] next to each test script shows success rate of all tests\n",
    "    * it will show you a failure report with detailed explanation (not here)\n",
    "        * lets fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile test_2.py\n",
    "#%%read test_2.py\n",
    "\n",
    "def test_sum():\n",
    "    assert sum([1,1]) == 3, \"Should be 2\"\n",
    "\n",
    "def test_len_vs__len__():\n",
    "    a_tuple = (1,2,3,5)\n",
    "    assert len(a_tuple) == a_tuple.__len__(), \"Function len returned differnt result than method __len__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* output next to the script indecates the status of each test:\n",
    "    * \".\" - test passed\n",
    "    * \"F\" - test failed\n",
    "    * \"E\" - test raised an unexcpected exception\n",
    "\n",
    "* it does not only show you the AssertionError though\n",
    "    * what does it show us (compared to the simple assert statement)?\n",
    "\n",
    "* if we want to run only some tests, we can specify which to ignore\n",
    "    * `--ignore`\n",
    "    * `--ignore-glob` - using glob (wildcard like patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tests/test_3.py\n",
    "\n",
    "def test_sum():\n",
    "    assert sum([1,1]) == 3, \"Should be 2\"\n",
    "\n",
    "def test_len_vs__len__():\n",
    "    a_tuple = (1,2,3,5)\n",
    "    assert len(a_tuple) == a_tuple.__len__(), \"Function len returned differnt result than method __len__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# checking where we are\n",
    "#!cd\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --ignore=tests/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when not ignoring\n",
    "!pytest --ignore-glob='*_3.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* in most modern code editors, managing a set of tests is more user friendly than from command line\n",
    "\n",
    "\n",
    "* tests often depend on:\n",
    "    * data\n",
    "    * test doubles\n",
    "* we don't want to mess with the originals => pytest **fixtures**\n",
    "\n",
    "### Fixtures\n",
    "* \"arranging\" part of the test\n",
    "\n",
    "* a method for providing:\n",
    "    * data\n",
    "    * test doubles\n",
    "    * state setup \n",
    "\n",
    "* more tests using the same underlying dataset → use fixture\n",
    "     * (repeating) data provided by a single function [decorated](#Decorators) with `@pytest.fixture`\n",
    "     \n",
    "* test depending on a fixture needs to have a fixture as an argument\n",
    "\n",
    "* let's look at the test double first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tests/test_fixture_smtp.py\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def smtp():\n",
    "    \"\"\"Initialize and return SMTP client session object\"\"\"\n",
    "    import smtplib\n",
    "    return smtplib.SMTP(\"smtp.gmail.com\")\n",
    "\n",
    "def test_ehlo(smtp):\n",
    "    \"\"\"Test response from sending Extended Helo (EHLO) is 250.\"\"\"\n",
    "    response, msg = smtp.ehlo()\n",
    "    assert response == 250\n",
    "    # assert 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest tests/test_fixture_smtp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* now fixture for providing data\n",
    "    * note: when providing path, think about the sourcedirectory! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tests/test_fixtures_data.py\n",
    "import pytest \n",
    "\n",
    "@pytest.fixture\n",
    "def data_names():\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('data/test_data_names.csv')\n",
    "    return df\n",
    "\n",
    "def test_addressing(data_names):\n",
    "    df = data_names\n",
    "    titles = df['Title']\n",
    "    surnames = df['Surname']\n",
    "    expected = df[['Addressing']]\n",
    "    assert (titles + ' ' + expected == surnames).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest tests/test_fixtures_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* when to avoid fixtures:\n",
    "    * using fixtures fixtures is as bad as using tests redundantly\n",
    "    *  => **marks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marks - test filtering\n",
    "\n",
    "* you might want to only run couple of your tests\n",
    "    * full suite of tests only sometimes\n",
    "    \n",
    "* to filter which tests to run:\n",
    "    * name-based filtering\n",
    "    * directory scoping \n",
    "    * **test categorization** (`-m` parameter)\n",
    "    \n",
    "* create **marks** (custom labels) to label any test you like (can have multiple labels)\n",
    "    * e.g. you can categorize your tests by dependencies (e.g. access to database - could be `@pytest.mark.database_access`\n",
    "* to run only tests in specific category (mark) `pytest -m <mark>`\n",
    "* to *not* run tests with specific mark `pytest -m \"not <mark>\"`\n",
    "\n",
    "* you should also [register the custom markers](https://stackoverflow.com/questions/60806473/pytestunknownmarkwarning-unknown-pytest-mark-xxx-is-this-a-typo) in *pytest.ini* file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tests/test_mark_example.py\n",
    "import pytest \n",
    "\n",
    "@pytest.mark.database\n",
    "def test_pg_read():\n",
    "    pass\n",
    "\n",
    "@pytest.mark.database\n",
    "def test_pg_write():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest -m database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* there are few marks out of the box:\n",
    "    * **skip** skips a test unconditionally\n",
    "    * **skipif** skips a test if the expression passed to it evaluates to True\n",
    "    * **parametrize** creates multiple variants of a test with different values as arguments\n",
    "    \n",
    "* you can see a list of all the marks pytest knows about by running `pytest --markers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest --markers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test parametrization\n",
    "\n",
    "* using only slightly different input and output would lead to repeating test definitions\n",
    "    * DRY!\n",
    "* fixtures not very good with only slightly different inputs and expected outputs\n",
    "    * **parametrize** a single test definition a get variants of the test for you with the parameters you specify\n",
    "    * mind the syntax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load tests/test_parametrize_example.py\n",
    "import pytest\n",
    "import unicodedata\n",
    "\n",
    "#######\n",
    "# Function we would like to test should be defined in package code, not here.\n",
    "########\n",
    "def drop_diacritics(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Strip accents from input String.\n",
    "    \n",
    "    :param text: The input string.\n",
    "    :returns: The processed string.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        raise TypeError(f'Input text should be a string, not %s', type(text))\n",
    "    \n",
    "    # Return the normal form for the Unicode string\n",
    "    # 'NFKD' stands for the normal form KD  \n",
    "    text = unicodedata.normalize('NFKD',text)\n",
    "    output = ''\n",
    "    \n",
    "    for char in text:\n",
    "        if not unicodedata.combining(char):\n",
    "            output += char\n",
    "            \n",
    "    return output\n",
    "#### \n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\"test_input,expected\", [(\"3+5\", 8), (\"2+4\", 6), (\"6*9\", 42)])\n",
    "def test_eval(test_input, expected):\n",
    "    assert eval(test_input) == expected\n",
    "    \n",
    "@pytest.mark.parametrize(\n",
    "    'original,output',\n",
    "    [\n",
    "        ('řeřicha', 'rericha'),\n",
    "        ('čeština', 'cestina')\n",
    "    ]\n",
    ") \n",
    "def test_drop_diacritics(original:str, output:str) -> None:\n",
    "    assert drop_diacritics(original) == output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pytest tests/test_parametrize_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing features to explore\n",
    "\n",
    "* [plugins](https://docs.pytest.org/en/latest/reference/plugin_list.html)\n",
    "    * requests-mock\n",
    "    * database-mock\n",
    "\n",
    "* [CI/CD](https://docs.github.com/en/actions/guides/about-continuous-integration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
